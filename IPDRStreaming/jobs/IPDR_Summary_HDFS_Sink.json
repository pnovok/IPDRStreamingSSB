{
  "job_name" : "IPDR_Summary_HDFS_Sink",
  "api_endpoints" : [ ],
  "sql" : "/*This SBB Job will create an HDFS sink for IPDR summary data and output the data there in CSV format */\nDROP TABLE IF EXISTS `ipdr_hdfs_sink`;\nCREATE TABLE  `ipdr_hdfs_sink` (\n  `window_start` TIMESTAMP(3) NOT NULL,\n  `window_end` TIMESTAMP(3) NOT NULL,\n  `cmMacAddr` VARCHAR(2147483647),\n  `upstream_oct` BIGINT,\n  `downstream_oct` BIGINT\n) WITH (\n  'connector' = 'filesystem', -- Specify what connector to use, for FileSystem it must be 'filesystem'.\n  'format' = 'csv', -- Data format\n  'path' = 'hdfs:///tmp/ipdr_summary' -- Path to the root directory of the table data.\n  -- 'auto-compaction' = 'false' -- Whether to enable automatic compaction in streaming sink or not. The data will be written to temporary files. After the checkpoint is completed, the temporary files generated by a checkpoint will be compacted. The temporary files are invisible before compaction.\n  -- 'compaction.file-size' = '...' -- The compaction target file size, the default value is the rolling file size.\n  -- 'csv.allow-comments' = 'false' -- Optional flag to ignore comment lines that start with \\\"#\\\" (disabled by default); if enabled, make sure to also ignore parse errors to allow empty rows\n  -- 'csv.array-element-delimiter' = ';' -- Optional array element delimiter string for separating array and row element values (\";\" by default)\n  -- 'csv.disable-quote-character' = 'false' -- Optional flag to disabled quote character for enclosing field values (false by default) if true, quote-character can not be set\n  -- 'csv.escape-character' = '...' -- Optional escape character for escaping values (disabled by default)\n  -- 'csv.field-delimiter' = ',' -- Optional field delimiter character (',' by default)\n  -- 'csv.ignore-parse-errors' = 'false' -- Optional flag to skip fields and rows with parse errors instead of failing; fields are set to null in case of errors\n  -- 'csv.null-literal' = '...' -- Optional null literal string that is interpreted as a null value (disabled by default)\n  -- 'csv.quote-character' = '\"' -- Optional quote character for enclosing field values ('\"' by default)\n  -- 'partition.default-name' = '...' -- Optional: default partition name in case the dynamic partition, column value is null/empty string.\n  -- 'partition.time-extractor.class' = '...' -- The extractor class for implement PartitionTimeExtractor interface.\n  -- 'partition.time-extractor.kind' = 'default' -- Time extractor to extract time from partition values. Support default and custom. For default, can configure timestamp pattern. For custom, should configure extractor class.\n  -- 'partition.time-extractor.timestamp-pattern' = '...' -- The 'default' construction way allows users to use partition fields to get a legal timestamp pattern. Default support 'yyyy-mm-dd hh:mm:ss' from first field. If timestamp should be extracted from a single partition field 'dt', can configure: '$dt'. If timestamp should be extracted from multiple partition fields, say 'year', 'month', 'day' and 'hour', can configure: '$year-$month-$day $hour:00:00'. If timestamp should be extracted from two partition fields 'dt' and 'hour', can configure: '$dt $hour:00:00'.\n  -- 'sink.parallelism' = '...' -- Parallelism of writing files into external file system. The value should greater than zero otherwise exception will be thrown.\n  -- 'sink.partition-commit.delay' = '0 s' -- The partition will not commit until the delay time. If it is a daily partition, should be '1 d', if it is a hourly partition, should be '1 h'.\n  -- 'sink.partition-commit.policy.class' = '...' -- The partition commit policy class for implement PartitionCommitPolicy interface. Only work in custom commit policy.\n  -- 'sink.partition-commit.policy.kind' = '...' -- Policy to commit a partition is to notify the downstream application that the partition has finished writing, the partition is ready to be read. metastore: add partition to metastore. Only hive table supports metastore policy, file system manages partitions through directory structure. success-file: add '_success' file to directory. Both can be configured at the same time: 'metastore,success-file'. custom: use policy class to create a commit policy. Support to configure multiple policies: 'metastore,success-file'.\n  -- 'sink.partition-commit.success-file.name' = '_SUCCESS' -- The file name for success-file partition commit policy.\n  -- 'sink.partition-commit.trigger' = 'process-time' -- Trigger type for partition commit: 'process-time': based on the time of the machine, it neither requires partition time extraction nor watermark generation. Commit partition once the 'current system time' passes 'partition creation system time' plus 'delay'. 'partition-time': based on the time that extracted from partition values, it requires watermark generation. Commit partition once the 'watermark' passes 'time extracted from partition values' plus 'delay'.\n  -- 'sink.partition-commit.watermark-time-zone' = 'UTC' -- The time zone to parse the long watermark value to TIMESTAMP value, the parsed watermark timestamp is used to compare with partition time to decide the partition should commit or not. This option is only take effect when `sink.partition-commit.trigger` is set to 'partition-time'. If this option is not configured correctly, e.g. source rowtime is defined on TIMESTAMP_LTZ column, but this config is not configured, then users may see the partition committed after a few hours. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is the session time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.\n  -- 'sink.rolling-policy.check-interval' = '1 m' -- The interval for checking time based rolling policies. This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'.\n  -- 'sink.rolling-policy.file-size' = '128MB' -- The maximum part file size before rolling.\n  -- 'sink.rolling-policy.rollover-interval' = '30 m' -- The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files). The frequency at which this is checked is controlled by the 'sink.rolling-policy.check-interval' option.\n  -- 'sink.shuffle-by-partition.enable' = 'false' -- Optional: the option to enable shuffle data by dynamic partition fields in sink phase, this can greatly reduce the number of file for filesystem sink but may lead data skew, the default value is false.\n);\n\n/* Get IPDR Summary Usage data from the view */\n/* I'm aggregating usOctets, dsOctets by Mac Address in 20 seconds windows and sending output to the filesystem*/\nINSERT INTO `ipdr_hdfs_sink` SELECT * FROM ipdr_summary;",
  "mv_config" : {
    "name" : "IPDR_Summary_HDFS_Sink",
    "retention" : 300,
    "min_row_retention_count" : 0,
    "recreate" : false,
    "key_column_name" : null,
    "column_indices_disabled" : false,
    "indexed_columns" : [ ],
    "not_indexed_columns" : [ ],
    "api_key" : null,
    "ignore_nulls" : false,
    "require_restart" : false,
    "batch_size" : 0,
    "enabled" : false
  },
  "runtime_config" : {
    "execution_mode" : "SESSION",
    "parallelism" : 1,
    "sample_interval" : 1000,
    "sample_count" : 100,
    "window_size" : 100,
    "start_with_savepoint" : false,
    "log_config" : {
      "type" : "LOG4J_PROPERTIES",
      "content" : "\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = MainAppender\n#Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\n\n# Log all infos in the given file\nappender.main.name = MainAppender\nappender.main.type = File\nappender.main.append = false\nappender.main.fileName = /var/log/ssb\nappender.main.layout.type = PatternLayout\nappender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\n"
    }
  }
}