{
  "job_name" : "IPDR_Summary_Kafka_Sink",
  "api_endpoints" : [ ],
  "sql" : "/*This SBB Job will create an Kafka sink for IPDR summary data and output the data there in JSON format */\n\nDROP TABLE IF EXISTS `ssb`.`IPDRStreaming`.`ipdr_kafka_sink`;\nCREATE TABLE  `ssb`.`IPDRStreaming`.`ipdr_kafka_sink` (\n  `window_start` TIMESTAMP(3),\n    `window_end` TIMESTAMP(3),\n  `cmMacAddr` VARCHAR(2147483647),\n  `dsScn` VARCHAR(2147483647),\n  `usage` BIGINT\n) WITH (\n  'connector' = 'kafka', -- Specify what connector to use, for Kafka it must use 'kafka'.\n  'format' = 'json', -- Data format\n  'properties.bootstrap.servers' = 'pnovokshonov-1.pnovokshonov.root.hwx.site:9092,pnovokshonov-2.pnovokshonov.root.hwx.site:9092,pnovokshonov-3.pnovokshonov.root.hwx.site:9092', -- Comma separated list of Kafka brokers.\n  'topic' = 'ipdr_output' -- To read data from when the table is used as source. It also supports topic list for source by separating topic by semicolon. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources. When the table is used as sink, the topic name is the topic to write data to. Note topic list is not supported for sinks.\n  -- 'json.encode.decimal-as-plain-number' = 'false' -- Optional flag to specify whether to encode all decimals as plain numbers instead of possible scientific notations, false by default.\n  -- 'json.fail-on-missing-field' = 'false' -- Optional flag to specify whether to fail if a field is missing or not, false by default.\n  -- 'json.ignore-parse-errors' = 'false' -- Optional flag to skip fields and rows with parse errors instead of failing; fields are set to null in case of errors, false by default.\n  -- 'json.map-null-key.literal' = 'null' -- Optional flag to specify string literal for null keys when 'map-null-key.mode' is LITERAL, \\\"null\\\" by default.\n  -- 'json.map-null-key.mode' = 'FAIL' -- Optional flag to control the handling mode when serializing null key for map data, FAIL by default. Option DROP will drop null key entries for map data. Option LITERAL will use 'map-null-key.literal' as key literal.\n  -- 'json.timestamp-format.standard' = 'SQL' -- Optional flag to specify timestamp format, SQL by default. Option ISO-8601 will parse input timestamp in \\\"yyyy-MM-ddTHH:mm:ss.s{precision}\\\" format and output timestamp in the same format. Option SQL will parse input timestamp in \\\"yyyy-MM-dd HH:mm:ss.s{precision}\\\" format and output timestamp in the same format.\n  -- 'key.fields' = '...' -- Defines an explicit list of physical columns from the table schema that configure the data type for the key format. By default, this list is empty and thus a key is undefined. The list should look like 'field1;field2'.\n  -- 'key.fields-prefix' = '...' -- Defines a custom prefix for all fields of the key format to avoid name clashes with fields of the value format. By default, the prefix is empty. If a custom prefix is defined, both the table schema and 'key.fields' will work with prefixed names. When constructing the data type of the key format, the prefix will be removed and the non-prefixed names will be used within the key format. Please note that this option requires that 'value.fields-include' must be set to 'EXCEPT_KEY'.\n  -- 'key.format' = '...' -- Specifies the format identifier for encoding key data. The format used to deserialize and serialize the key part of Kafka messages. Note: If a key format is defined, the 'key.fields' option is required as well. Otherwise the Kafka records will have an empty key.\n  -- 'properties.*' = '...' -- This can set and pass arbitrary Kafka configurations. Suffix names must match the configuration key defined in Kafka Configuration documentation. Flink will remove the \"properties.\" key prefix and pass the transformed key and values to the underlying KafkaClient. For example, you can disable automatic topic creation via 'properties.allow.auto.create.topics' = 'false'. But there are some configurations that do not support to set, because Flink will override them, e.g. 'key.deserializer' and 'value.deserializer'.\n  -- 'properties.group.id' = '...' -- The id of the consumer group for Kafka source. If group ID is not specified, an automatically generated id 'KafkaSource-{tableIdentifier}' will be used.\n  -- 'scan.startup.mode' = 'group-offsets' -- Startup mode for Kafka consumer, valid values are 'earliest-offset', 'latest-offset', 'group-offsets', 'timestamp' and 'specific-offsets'\n  -- 'scan.startup.specific-offsets' = '...' -- Specify offsets for each partition in case of 'specific-offsets' startup mode, e.g. 'partition:0,offset:42;partition:1,offset:300'.\n  -- 'scan.startup.timestamp-millis' = '...' -- Start from the specified epoch timestamp (milliseconds) used in case of 'timestamp' startup mode.\n  -- 'scan.topic-partition-discovery.interval' = '...' -- Interval for consumer to discover dynamically created Kafka topics and partitions periodically.\n  -- 'sink.delivery-guarantee' = 'at-least-once' -- Defines the delivery semantic for the Kafka sink. Valid enumerations are 'at-least-once', 'exactly-once' and 'none'.\n  -- 'sink.parallelism' = '...' -- Defines the parallelism of the Kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.\n -- 'sink.parallelism' = '2'\n  -- 'sink.partitioner' = 'default' -- Output partitioning from Flink's partitions into Kafka's partitions. Valid values are: 1) default(use the kafka default partitioner to partition records), 2) fixed(each Flink partition ends up in at most one Kafka partition), 3) round-robin(a Flink partition is distributed to Kafka partitions sticky round-robin. It only works when record's keys are not specified), 4) Custom FlinkKafkaPartitioner subclass(e.g. 'org.mycompany.MyPartitioner')\n  -- 'sink.transactional-id-prefix' = '...' -- If the delivery guarantee is configured as 'exactly-once' this value must be set and is used a prefix for the identifier of all opened Kafka transactions.\n  -- 'topic-pattern' = '...' -- The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources.\n  -- 'value.fields-include' = 'ALL' -- Defines a strategy how to deal with key columns in the data type of the value format. By default, 'ALL' physical columns of the table schema will be included in the value format which means that key columns appear in the data type for both the key and value format.\n  -- 'value.format' = 'true' -- Specifies the format identifier for encoding value data. The format used to deserialize and serialize the value part of Kafka messages. Note: Either this option or the 'format' option are required.\n);\n\n\n\n/* Get IPDR Summary Usage data from the view */\n/* I'm aggregating usOctets, dsOctets by Mac Address in 20 seconds windows and sending output to the filesystem*/\nINSERT INTO `ipdr_kafka_sink` SELECT * FROM ipdr_summary;",
  "mv_config" : {
    "name" : "IPDR_Summary_Kafka_Sink",
    "retention" : 300,
    "min_row_retention_count" : 0,
    "recreate" : false,
    "key_column_name" : null,
    "column_indices_disabled" : false,
    "indexed_columns" : [ ],
    "not_indexed_columns" : [ ],
    "api_key" : null,
    "ignore_nulls" : false,
    "require_restart" : false,
    "batch_size" : 0,
    "enabled" : false
  },
  "runtime_config" : {
    "execution_mode" : "SESSION",
    "parallelism" : 1,
    "sample_interval" : -1,
    "sample_count" : 100,
    "window_size" : 100,
    "start_with_savepoint" : false,
    "log_config" : {
      "type" : "LOG4J_PROPERTIES",
      "content" : "\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = MainAppender\n#Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\n\n# Log all infos in the given file\nappender.main.name = MainAppender\nappender.main.type = File\nappender.main.append = false\nappender.main.fileName = /var/log/ssb\nappender.main.layout.type = PatternLayout\nappender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\n"
    }
  }
}